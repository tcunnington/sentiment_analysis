{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All below in progress...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_graph(data_dto, x, seqlens, y_true, optimizer, loss, accuracy, num_epochs=20, feed_extra={}):\n",
    "\n",
    "    # TODO do not use global vars!\n",
    "    data_name = data_dto.name\n",
    "    x_train, x_test = data_dto.x_train, data_dto.x_test\n",
    "    y_train, y_test = data_dto.y_train, data_dto.y_test\n",
    "    lengths_train, lengths_test = data_dto.l_train, data_dto.l_test\n",
    "    \n",
    "    print('Using data: ' + data_name)\n",
    "    \n",
    "    reset_vars()\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc' : [],\n",
    "    }\n",
    "    print_every = num_epochs // 10\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        shuffle_idxs = np.arange(len(x_train))\n",
    "        np.random.shuffle(shuffle_idxs)\n",
    "\n",
    "        x_train = x_train[shuffle_idxs]\n",
    "        y_train = y_train[shuffle_idxs]\n",
    "        lengths_train = lengths_train[shuffle_idxs]\n",
    "\n",
    "        metrics['train_loss'].append(0)\n",
    "        metrics['train_acc'].append(0)\n",
    "\n",
    "        num_steps = len(x_train) // batch_size\n",
    "\n",
    "        # loop through train data in batches\n",
    "        for j in range(num_steps):\n",
    "\n",
    "            start, end = j*batch_size, (j+1)*batch_size\n",
    "\n",
    "            train_feed = {\n",
    "                x: x_train[start:end],\n",
    "                y_true: y_train[start:end],\n",
    "                seqlens: lengths_train[start:end],\n",
    "            }\n",
    "            train_feed.update(feed_extra)\n",
    "            \n",
    "            sess.run(optimizer, feed_dict=train_feed)\n",
    "            l, a = sess.run([loss, accuracy], feed_dict=train_feed)\n",
    "            metrics['train_loss'][i] += l\n",
    "            metrics['train_acc'][i] += a\n",
    "\n",
    "        # calculate train metrics\n",
    "        metrics['train_loss'][i] /= num_steps\n",
    "        metrics['train_acc'][i] /= num_steps\n",
    "\n",
    "        # prep test loop\n",
    "        num_test_steps = len(x_test) // batch_size     # TODO this leaves out the last few data points..\n",
    "        metrics['test_loss'].append(0)\n",
    "        metrics['test_acc'].append(0)\n",
    "\n",
    "        for k in range(num_test_steps):\n",
    "            start, end = k*batch_size, (k+1)*batch_size\n",
    "            \n",
    "            test_feed = {\n",
    "                x: x_test[start:end],\n",
    "                y_true: y_test[start:end],\n",
    "                seqlens: lengths_test[start:end]\n",
    "            }\n",
    "            test_feed.update(feed_extra)\n",
    "            \n",
    "            tl, ta = sess.run([loss, accuracy], feed_dict=test_feed)\n",
    "            metrics['test_loss'][i] += tl\n",
    "            metrics['test_acc'][i] += ta\n",
    "\n",
    "        metrics['test_loss'][i] /= num_test_steps\n",
    "        metrics['test_acc'][i] /= num_test_steps\n",
    "\n",
    "#         print(i, i % print_every, print_every, num_epochs)\n",
    "        if i % print_every == 0 or i == (num_epochs - 1):\n",
    "            print(\"(epoch %i)\\t Train: %0.5f, %0.5f \\tTest: %0.5f, %0.5f\" % (i, metrics['train_loss'][i], metrics['train_acc'][i], metrics['test_loss'][i], metrics['test_acc'][i]))\n",
    "        \n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(batch_size=10, \n",
    "                rnn_size=25, \n",
    "                embedding_size=64,\n",
    "                dropout_keepprob=0.8):\n",
    "\n",
    "    reset_tf()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, shape=(batch_size, PAD_SIZE), name='x') # as indices of embedding\n",
    "    seqlens = tf.placeholder(tf.int32, shape=[batch_size], name='seqlens')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[batch_size], name='y_true')\n",
    "\n",
    "    # Start with embedding layer\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "    embedding_input = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN - try also BasicRNNCell, GRUCell, BasicLSTMCell\n",
    "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "    # Iteratively compute output of recurrent network\n",
    "    rnn_out, lstm_state = tf.nn.dynamic_rnn(rnn_cell, embedding_input,\n",
    "                                            sequence_length=seqlens, dtype=tf.float32)\n",
    "\n",
    "    rnn_out = tf.nn.dropout(rnn_out, dropout_keepprob)\n",
    "\n",
    "    # Get single output accoring to each sequence length\n",
    "    out = tf.gather_nd(rnn_out, tf.stack([tf.range(batch_size), seqlens-1], axis=1))\n",
    "\n",
    "    # Linear activation (FC layer on top of the LSTM net)\n",
    "    y = tf.layers.dense(out, 1, activation=None)\n",
    "    y = tf.reshape(y,(batch_size,))\n",
    "    \n",
    "    return x, seqlens, y, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build it, create metrics, run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "x, seqlens, y, y_true = build_graph(rnn_size=12, embedding_size=10, dropout_keepprob=0.7)\n",
    "\n",
    "preds = tf.nn.softmax(y)\n",
    "label_predictions = preds > 0.5\n",
    "correct = tf.equal(tf.cast(label_predictions, tf.int32), tf.cast(y_true, tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "ETA = 0.01\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_true))\n",
    "optimizer = tf.train.AdamOptimizer(ETA).minimize(loss) \n",
    "# optimizer = tf.train.RMSPropOptimizer(ETA).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics = run_graph(data_gensim_dct, x, seqlens, y_true, optimizer, loss, accuracy, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Better than chance, but likely the model is hindered by the small amount of training data and the sparsity of words--after all the embedding has no way to use context and thus won't build very meaningful relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Spacy's GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of embedding matrix, Gb\n",
    "nlp.vocab.vectors.data.nbytes / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude words that don't have a vector for now\n",
    "# get index of vector, not vector itself\n",
    "def sent2seq_glove(sent,nlp): \n",
    "    return [nlp.vocab[w].rank for w in sent.split() if nlp.vocab[w].has_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild data using spacy's indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seqlen_glove'] = [len(sent2seq_glove(s,nlp)) for s in df['clean']]\n",
    "df['is_valid_seq_glove'] = df['seqlen_glove'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'spacy indexes'\n",
    "\n",
    "data, lengths = pad([sent2seq_glove(s,nlp) for s in df[df['is_valid_seq_glove']]['clean']])\n",
    "y_labels = np.array(df[df['is_valid_seq_glove']]['score'])\n",
    "\n",
    "print(data.shape, lengths.shape, y_labels.shape)\n",
    "\n",
    "# do test train split\n",
    "split_idxs = np.random.random(len(df)) < 0.8\n",
    "\n",
    "x_train = data[split_idxs]\n",
    "y_train = y_labels[split_idxs]\n",
    "lengths_train = lengths[split_idxs]\n",
    "\n",
    "x_test = data[~split_idxs]\n",
    "y_test = y_labels[~split_idxs]\n",
    "lengths_test = lengths[~split_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seqlen_glove'].mean(), df['seqlen_glove'].median(), (~df['is_valid_seq_glove']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fortunately this embedding is able to capture more words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph is nearly the exact same except for the embedding layer. It is no longer learned, but constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT_SIZE = 300\n",
    "\n",
    "def build_graph_glove(batch_size=10, \n",
    "                     rnn_size=25,\n",
    "                     dropout_keepprob=0.8):\n",
    "\n",
    "    reset_tf()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, shape=(batch_size, PAD_SIZE), name='x') # as indices of embedding\n",
    "    seqlens = tf.placeholder(tf.int32, shape=[batch_size], name='seqlens')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[batch_size], name='y_true')\n",
    "\n",
    "    # Start with embedding layer\n",
    "    embedding_matrix = tf.placeholder(shape=nlp.vocab.vectors.data.shape, \n",
    "                                      dtype=tf.float32, name='embedding_matrix')\n",
    "    rnn_input = tf.nn.embedding_lookup(embedding_matrix, x)\n",
    "    \n",
    "    # optional dense layer...\n",
    "#     rnn_input = tf.layers.dense(embedding_input, 64, activation=tf.nn.elu)\n",
    "\n",
    "    # RNN - try also BasicRNNCell, GRUCell, BasicLSTMCell\n",
    "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "    # Iteratively compute output of recurrent network\n",
    "    rnn_out, lstm_state = tf.nn.dynamic_rnn(rnn_cell, rnn_input, \n",
    "                                            sequence_length=seqlens, dtype=tf.float32)\n",
    "\n",
    "    rnn_out = tf.nn.dropout(rnn_out, dropout_keepprob)\n",
    "\n",
    "    # Get single output accoring to each sequence length\n",
    "    out = tf.gather_nd(rnn_out, tf.stack([tf.range(batch_size), seqlens-1], axis=1))\n",
    "\n",
    "    # Linear activation (FC layer on top of the LSTM net)\n",
    "    y = tf.layers.dense(out, 1, activation=None)\n",
    "    y = tf.reshape(y,(batch_size,))\n",
    "    \n",
    "    return x, seqlens, y, y_true, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, seqlens, y, y_true, embedding_matrix = build_graph_glove(rnn_size=12, dropout_keepprob=0.7) #batch_size=\n",
    "\n",
    "preds = tf.nn.softmax(y)\n",
    "label_predictions = preds > 0.5\n",
    "correct = tf.equal(tf.cast(label_predictions, tf.int32), tf.cast(y_true, tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "ETA = 0.01\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_true))\n",
    "optimizer = tf.train.AdamOptimizer(ETA).minimize(loss) \n",
    "# optimizer = tf.train.RMSPropOptimizer(ETA).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_metrics = run_graph(x, seqlens, y_true, optimizer, loss, accuracy, num_epochs=20, \n",
    "                        feed_extra={embedding_matrix:nlp.vocab.vectors.data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No great improvement (but at least it's not eating massive amount of memory now!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT_SIZE = 300\n",
    "\n",
    "def build_graph_bidir(batch_size=10, \n",
    "                      rnn_size=25, \n",
    "                      embedding_size=64,\n",
    "                      dropout_keepprob=0.8):\n",
    "\n",
    "    reset_tf()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, shape=(batch_size, PAD_SIZE), name='x') # as indices of embedding\n",
    "    seqlens = tf.placeholder(tf.int32, shape=[batch_size], name='seqlens')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[batch_size], name='y_true')\n",
    "\n",
    "#     # Start with embedding layer\n",
    "#     embeddings = tf.Variable(nlp.vocab.vectors.data, trainable=False)\n",
    "#     embedding_input = tf.nn.embedding_lookup(embeddings, x)\n",
    "    # Start with embedding layer\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "    embedding_input = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN - try also BasicRNNCell, GRUCell, BasicLSTMCell\n",
    "    rnn_fw = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    rnn_bw = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "    rnn_outs, rnn_states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                cell_fw=rnn_fw,\n",
    "                                cell_bw=rnn_bw,\n",
    "                                inputs=embedding_input,\n",
    "                                sequence_length=seqlens, dtype=tf.float32)\n",
    " \n",
    "    out_fw, out_bw = rnn_outs\n",
    "    state_fw, state_bw = rnn_states\n",
    "    \n",
    "    out_fw = tf.gather_nd(out_fw, tf.stack([tf.range(batch_size), seqlens-1], axis=1))\n",
    "    out_bw = tf.gather_nd(out_bw, tf.stack([tf.range(batch_size), seqlens-1], axis=1))\n",
    "    \n",
    "#     print(out_fw)\n",
    "    rnn_out = tf.concat([out_fw, out_bw], axis=1)\n",
    "#     print(rnn_out)\n",
    "    \n",
    "    out = tf.nn.dropout(rnn_out, dropout_keepprob)    \n",
    "\n",
    "    # Linear activation (FC layer on top of the LSTM net)\n",
    "    y = tf.layers.dense(out, 1, activation=None)\n",
    "    y = tf.reshape(y,(batch_size,))\n",
    "    \n",
    "    return x, seqlens, y, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, seqlens, y, y_true = build_graph_bidir(rnn_size=8, embedding_size=10, dropout_keepprob=0.7) #batch_size=\n",
    "\n",
    "preds = tf.nn.softmax(y)\n",
    "label_predictions = preds > 0.5\n",
    "correct = tf.equal(tf.cast(label_predictions, tf.int32), tf.cast(y_true, tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "ETA = 0.01\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_true))\n",
    "optimizer = tf.train.AdamOptimizer(ETA).minimize(loss) \n",
    "# optimizer = tf.train.RMSPropOptimizer(ETA).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics = run_graph(x, seqlens, y_true, optimizer, loss, accuracy, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A little improvment over a single LSTM here, but once I move on to longer text it may begin to make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
