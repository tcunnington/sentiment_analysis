{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Taylor/anaconda2/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Proof of concept data: Data is sentences from reviews on Yelp, IMDB, and Amazon. All sentences are labelled positive or negative--there's meant to be no neutral sentences in the data. Sentiment is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/sentiment_sentences/'\n",
    "file_names = ['amazon_cells_labelled.txt','imdb_labelled.txt','yelp_labelled.txt']\n",
    "\n",
    "def read_data(file_name):\n",
    "    return pd.read_csv(os.path.join(data_dir, file_name), sep='\\t', header=None, quoting=csv.QUOTE_NONE)\\\n",
    "        .rename(columns={\n",
    "            0: 'sentence',\n",
    "            1: 'score',\n",
    "        })\n",
    "df = pd.concat([read_data(f) for f in file_names]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So there is no way for me to plug it in here in the US unless I go by a converter.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data set is small and we have to split to train. May not be enough to get good results. Lets find out how many words per sentence on avg..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean, remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>so there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>good case excellent value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>great for the jawbone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>the mic is great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score  \\\n",
       "0  So there is no way for me to plug it in here i...      0   \n",
       "1                        Good case, Excellent value.      1   \n",
       "2                             Great for the jawbone.      1   \n",
       "3  Tied to charger for conversations lasting more...      0   \n",
       "4                                  The mic is great.      1   \n",
       "\n",
       "                                               clean  \n",
       "0  so there is no way for me to plug it in here i...  \n",
       "1                          good case excellent value  \n",
       "2                              great for the jawbone  \n",
       "3  tied to charger for conversations lasting more...  \n",
       "4                                   the mic is great  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import strip_numeric, strip_punctuation, strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "# prep_filters = [strip_punctuation, strip_numeric, strip_multiple_whitespaces]\n",
    "prep_filters = [strip_punctuation, strip_multiple_whitespaces]\n",
    "\n",
    "df['clean'] = df['sentence'].map(lambda s: ' '.join(preprocess_string(s.lower(), prep_filters)))\n",
    "# ' '.join(preprocess_string(df.head(1)['sentence'][0], prep_filters))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Baseline: bag of words model with LR or NB classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with learned word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [sent.split() for sent in df['clean']]\n",
    "dct = Dictionary(corpus)\n",
    "\n",
    "dct.filter_extremes(no_below=5)\n",
    "dct.compactify()\n",
    "\n",
    "vocab_size = len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude words that don't exist so we don't have to think about how to embed them :)\n",
    "def sent2seq(sent, dct): \n",
    "    return [idx for idx in dct.doc2idx(sent.split()) if idx != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 14, 7, 10, 18, 2, 9, 15, 11, 8, 6, 4, 6, 13, 17, 16, 5, 3, 1, 0]\n",
      "[21, 19, 20, 22]\n",
      "[23, 2, 13]\n",
      "[15, 24, 2, 26, 28, 25, 27]\n",
      "[13, 7, 23]\n",
      "[5, 31, 15, 13, 11, 15, 30, 8, 15, 32, 34, 33, 15, 30, 29, 35]\n",
      "[38, 45, 31, 42, 41, 42, 44, 13, 37, 39, 36, 39, 43, 40, 1, 40]\n",
      "[38, 45, 46, 49, 48, 45, 47, 31, 50]\n",
      "[15, 53, 5, 54, 52, 51]\n",
      "[58, 0, 57, 39, 51, 55, 56]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sample_sentence = df['clean'][i]\n",
    "    seq = sent2seq(sample_sentence, dct)\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['seqlen'] = [len(sent2seq(s, dct)) for s in df['clean']]\n",
    "df['is_valid_seq_gensim'] = df['seqlen'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.934, 9.0, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['seqlen'].mean(), df['seqlen'].median(), (~df['is_valid_seq_gensim']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A typical sequence is about 9 words. Is this large? It seems like it may be too small given the limited number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OUTPUT_SIZE = 2\n",
    "PAD_SIZE = 40 # 99th percentile (longer ones truncated)\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(xs, pad_size=PAD_SIZE): # pass in as list, since next dim is not fixed size\n",
    "    padded = np.zeros([len(xs), pad_size])\n",
    "    lens = np.zeros(len(xs), dtype=np.int32)\n",
    "    for i,vec in enumerate(xs): # by row\n",
    "        if len(vec) > pad_size:\n",
    "            vec = vec[:pad_size]\n",
    "        \n",
    "        padded[i,:len(vec)] = vec\n",
    "        lens[i] = len(vec)\n",
    "        \n",
    "    return padded, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2993, 40) (2993,) (2993,)\n"
     ]
    }
   ],
   "source": [
    "data_name = 'gensim indexes'\n",
    "\n",
    "data, lengths = pad([sent2seq(s,dct) for s in df[df['is_valid_seq_gensim']]['clean']])\n",
    "y_labels = np.array(df[df['is_valid_seq_gensim']]['score'])\n",
    "\n",
    "print(data.shape, lengths.shape, y_labels.shape)\n",
    "\n",
    "# do test train split\n",
    "split_idxs = np.random.random(len(data)) < 0.8\n",
    "\n",
    "x_train = data[split_idxs]\n",
    "y_train = y_labels[split_idxs]\n",
    "lengths_train = lengths[split_idxs]\n",
    "\n",
    "x_test = data[~split_idxs]\n",
    "y_test = y_labels[~split_idxs]\n",
    "lengths_test = lengths[~split_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Graph runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run_graph(x, seqlens, y_true, optimizer, loss, accuracy, num_epochs=20, feed_extra={}):\n",
    "\n",
    "    # TODO do not use global vars!\n",
    "    global data_name\n",
    "    global x_train, y_train\n",
    "    global x_test, y_test\n",
    "    global lengths_train, lengths_test\n",
    "    \n",
    "    print('Using global data! - ' + data_name)\n",
    "    \n",
    "    reset_vars()\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc' : [],\n",
    "    }\n",
    "    print_every = num_epochs // 10\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        shuffle_idxs = np.arange(len(x_train))\n",
    "        np.random.shuffle(shuffle_idxs)\n",
    "\n",
    "        x_train = x_train[shuffle_idxs]\n",
    "        y_train = y_train[shuffle_idxs]\n",
    "        lengths_train = lengths_train[shuffle_idxs]\n",
    "\n",
    "        metrics['train_loss'].append(0)\n",
    "        metrics['train_acc'].append(0)\n",
    "\n",
    "        num_steps = len(x_train) // batch_size\n",
    "\n",
    "        # loop through train data in batches\n",
    "        for j in range(num_steps):\n",
    "\n",
    "            start, end = j*batch_size, (j+1)*batch_size\n",
    "\n",
    "            train_feed = {\n",
    "                x: x_train[start:end],\n",
    "                y_true: y_train[start:end],\n",
    "                seqlens: lengths_train[start:end],\n",
    "            }\n",
    "            train_feed.update(feed_extra)\n",
    "            \n",
    "            sess.run(optimizer, feed_dict=train_feed)\n",
    "            l, a = sess.run([loss, accuracy], feed_dict=train_feed)\n",
    "            metrics['train_loss'][i] += l\n",
    "            metrics['train_acc'][i] += a\n",
    "\n",
    "        # calculate train metrics\n",
    "        metrics['train_loss'][i] /= num_steps\n",
    "        metrics['train_acc'][i] /= num_steps\n",
    "\n",
    "        # prep test loop\n",
    "        num_test_steps = len(x_test) // batch_size     # TODO this leaves out the last few data points..\n",
    "        metrics['test_loss'].append(0)\n",
    "        metrics['test_acc'].append(0)\n",
    "\n",
    "        for k in range(num_test_steps):\n",
    "            start, end = k*batch_size, (k+1)*batch_size\n",
    "            \n",
    "            test_feed = {\n",
    "                x: x_test[start:end],\n",
    "                y_true: y_test[start:end],\n",
    "                seqlens: lengths_test[start:end]\n",
    "            }\n",
    "            test_feed.update(feed_extra)\n",
    "            \n",
    "            tl, ta = sess.run([loss, accuracy], feed_dict=test_feed)\n",
    "            metrics['test_loss'][i] += tl\n",
    "            metrics['test_acc'][i] += ta\n",
    "\n",
    "        metrics['test_loss'][i] /= num_test_steps\n",
    "        metrics['test_acc'][i] /= num_test_steps\n",
    "\n",
    "#         print(i, i % print_every, print_every, num_epochs)\n",
    "        if i % print_every == 0 or i == (num_epochs - 1):\n",
    "            print(\"(epoch %i)\\t Train: %0.5f, %0.5f \\tTest: %0.5f, %0.5f\" % (i, metrics['train_loss'][i], metrics['train_acc'][i], metrics['test_loss'][i], metrics['test_acc'][i]))\n",
    "        \n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(batch_size=10, \n",
    "                rnn_size=25, \n",
    "                embedding_size=64,\n",
    "                dropout_keepprob=0.8):\n",
    "\n",
    "    reset_tf()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, shape=(batch_size, PAD_SIZE), name='x') # as indices of embedding\n",
    "    seqlens = tf.placeholder(tf.int32, shape=[batch_size], name='seqlens')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[batch_size], name='y_true')\n",
    "\n",
    "    # Start with embedding layer\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "    embedding_input = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN - try also BasicRNNCell, GRUCell, BasicLSTMCell\n",
    "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "    # Iteratively compute output of recurrent network\n",
    "    rnn_out, lstm_state = tf.nn.dynamic_rnn(rnn_cell, embedding_input,\n",
    "                                            sequence_length=seqlens, dtype=tf.float32)\n",
    "\n",
    "    rnn_out = tf.nn.dropout(rnn_out, dropout_keepprob)\n",
    "\n",
    "    # Get single output accoring to each sequence length\n",
    "    out = tf.gather_nd(rnn_out, tf.stack([tf.range(batch_size), seqlens-1], axis=1))\n",
    "\n",
    "    # Linear activation (FC layer on top of the LSTM net)\n",
    "    y = tf.layers.dense(out, 1, activation=None)\n",
    "    y = tf.reshape(y,(batch_size,))\n",
    "    \n",
    "    return x, seqlens, y, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build it, create metrics, run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "x, seqlens, y, y_true = build_graph(rnn_size=12, embedding_size=10, dropout_keepprob=0.7)\n",
    "\n",
    "preds = tf.nn.softmax(y)\n",
    "label_predictions = preds > 0.5\n",
    "correct = tf.equal(tf.cast(label_predictions, tf.int32), tf.cast(y_true, tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "ETA = 0.01\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_true))\n",
    "optimizer = tf.train.AdamOptimizer(ETA).minimize(loss) \n",
    "# optimizer = tf.train.RMSPropOptimizer(ETA).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global data! - gensim indexes\n",
      "(epoch 0)\t Train: 0.59145, 0.50000 \tTest: 0.53654, 0.52031\n",
      "(epoch 2)\t Train: 0.27829, 0.56766 \tTest: 0.59447, 0.55000\n",
      "(epoch 4)\t Train: 0.17064, 0.58085 \tTest: 0.56564, 0.57500\n",
      "(epoch 6)\t Train: 0.11808, 0.58340 \tTest: 0.66359, 0.58594\n",
      "(epoch 8)\t Train: 0.08761, 0.58936 \tTest: 0.83225, 0.57813\n",
      "(epoch 10)\t Train: 0.07510, 0.58596 \tTest: 0.86325, 0.58594\n",
      "(epoch 12)\t Train: 0.06223, 0.58979 \tTest: 1.07629, 0.56406\n",
      "(epoch 14)\t Train: 0.05084, 0.58681 \tTest: 1.09434, 0.56875\n",
      "(epoch 16)\t Train: 0.04171, 0.58936 \tTest: 1.10326, 0.59063\n",
      "(epoch 18)\t Train: 0.02981, 0.58936 \tTest: 1.26463, 0.58125\n",
      "(epoch 19)\t Train: 0.04814, 0.59106 \tTest: 1.16361, 0.57969\n"
     ]
    }
   ],
   "source": [
    "run_metrics = run_graph(x, seqlens, y_true, optimizer, loss, accuracy, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Better than chance, but likely the model is hindered by the small amount of training data and the sparsity of words--after all the embedding has no way to use context and thus won't build very meaningful relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Spacy's GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1969033628702164"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of embedding matrix, Gb\n",
    "nlp.vocab.vectors.data.nbytes / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude words that don't have a vector for now\n",
    "def sent2seq_glove(sent,nlp): \n",
    "    return [nlp.vocab[w].rank for w in sent.split() if nlp.vocab[w].has_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild data using spacy's indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seqlen_glove'] = [len(sent2seq_glove(s,nlp)) for s in df['clean']]\n",
    "df['is_valid_seq_glove'] = df['seqlen_glove'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 40) (3000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "data_name = 'spacy indexes'\n",
    "\n",
    "data, lengths = pad([sent2seq_glove(s,nlp) for s in df[df['is_valid_seq_glove']]['clean']])\n",
    "y_labels = np.array(df[df['is_valid_seq_glove']]['score'])\n",
    "\n",
    "print(data.shape, lengths.shape, y_labels.shape)\n",
    "\n",
    "# do test train split\n",
    "split_idxs = np.random.random(len(df)) < 0.8\n",
    "\n",
    "x_train = data[split_idxs]\n",
    "y_train = y_labels[split_idxs]\n",
    "lengths_train = lengths[split_idxs]\n",
    "\n",
    "x_test = data[~split_idxs]\n",
    "y_test = y_labels[~split_idxs]\n",
    "lengths_test = lengths[~split_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.088333333333333, 10.0, 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['seqlen_glove'].mean(), df['seqlen_glove'].median(), (~df['is_valid_seq_glove']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fortunately this embedding is able to capture more words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph is nearly the exact same except for the embedding layer. It is no longer learned, but constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT_SIZE = 300\n",
    "\n",
    "def build_graph_glove(batch_size=10, \n",
    "                     rnn_size=25,\n",
    "                     dropout_keepprob=0.8):\n",
    "\n",
    "    reset_tf()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, shape=(batch_size, PAD_SIZE), name='x') # as indices of embedding\n",
    "    seqlens = tf.placeholder(tf.int32, shape=[batch_size], name='seqlens')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[batch_size], name='y_true')\n",
    "\n",
    "    # Start with embedding layer\n",
    "    embedding_matrix = tf.placeholder(shape=nlp.vocab.vectors.data.shape, \n",
    "                                      dtype=tf.float32, name='embedding_matrix')\n",
    "    rnn_input = tf.nn.embedding_lookup(embedding_matrix, x)\n",
    "    \n",
    "    # optional dense layer...\n",
    "#     rnn_input = tf.layers.dense(embedding_input, 64, activation=tf.nn.elu)\n",
    "\n",
    "    # RNN - try also BasicRNNCell, GRUCell, BasicLSTMCell\n",
    "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "    # Iteratively compute output of recurrent network\n",
    "    rnn_out, lstm_state = tf.nn.dynamic_rnn(rnn_cell, rnn_input, \n",
    "                                            sequence_length=seqlens, dtype=tf.float32)\n",
    "\n",
    "    rnn_out = tf.nn.dropout(rnn_out, dropout_keepprob)\n",
    "\n",
    "    # Get single output accoring to each sequence length\n",
    "    out = tf.gather_nd(rnn_out, tf.stack([tf.range(batch_size), seqlens-1], axis=1))\n",
    "\n",
    "    # Linear activation (FC layer on top of the LSTM net)\n",
    "    y = tf.layers.dense(out, 1, activation=None)\n",
    "    y = tf.reshape(y,(batch_size,))\n",
    "    \n",
    "    return x, seqlens, y, y_true, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, seqlens, y, y_true, embedding_matrix = build_graph_glove(rnn_size=12, dropout_keepprob=0.7) #batch_size=\n",
    "\n",
    "preds = tf.nn.softmax(y)\n",
    "label_predictions = preds > 0.5\n",
    "correct = tf.equal(tf.cast(label_predictions, tf.int32), tf.cast(y_true, tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "ETA = 0.01\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_true))\n",
    "optimizer = tf.train.AdamOptimizer(ETA).minimize(loss) \n",
    "# optimizer = tf.train.RMSPropOptimizer(ETA).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global data! - spacy indexes\n",
      "(epoch 0)\t Train: 0.40510, 0.53792 \tTest: 0.35100, 0.50847\n",
      "(epoch 2)\t Train: 0.18295, 0.58125 \tTest: 0.32369, 0.54576\n",
      "(epoch 4)\t Train: 0.11481, 0.59000 \tTest: 0.41696, 0.55593\n",
      "(epoch 6)\t Train: 0.08695, 0.59292 \tTest: 0.41609, 0.54915\n",
      "(epoch 8)\t Train: 0.06195, 0.59500 \tTest: 0.52518, 0.55424\n",
      "(epoch 10)\t Train: 0.04288, 0.59792 \tTest: 0.59999, 0.54237\n",
      "(epoch 12)\t Train: 0.04089, 0.59625 \tTest: 0.47918, 0.55593\n",
      "(epoch 14)\t Train: 0.08053, 0.59500 \tTest: 0.54101, 0.54915\n",
      "(epoch 16)\t Train: 0.04207, 0.59833 \tTest: 0.53253, 0.55932\n",
      "(epoch 18)\t Train: 0.03377, 0.59667 \tTest: 0.54245, 0.55593\n",
      "(epoch 19)\t Train: 0.04224, 0.59875 \tTest: 0.64583, 0.55254\n"
     ]
    }
   ],
   "source": [
    "run_metrics = run_graph(x, seqlens, y_true, optimizer, loss, accuracy, num_epochs=20, \n",
    "                        feed_extra={embedding_matrix:nlp.vocab.vectors.data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No great improvement (but at least it's not eating massive amount of memory now!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INPUT_SIZE = 300\n",
    "\n",
    "def build_graph_bidir(batch_size=10, \n",
    "                      rnn_size=25, \n",
    "                      embedding_size=64,\n",
    "                      dropout_keepprob=0.8):\n",
    "\n",
    "    reset_tf()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, shape=(batch_size, PAD_SIZE), name='x') # as indices of embedding\n",
    "    seqlens = tf.placeholder(tf.int32, shape=[batch_size], name='seqlens')\n",
    "    y_true = tf.placeholder(tf.float32, shape=[batch_size], name='y_true')\n",
    "\n",
    "#     # Start with embedding layer\n",
    "#     embeddings = tf.Variable(nlp.vocab.vectors.data, trainable=False)\n",
    "#     embedding_input = tf.nn.embedding_lookup(embeddings, x)\n",
    "    # Start with embedding layer\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "    embedding_input = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN - try also BasicRNNCell, GRUCell, BasicLSTMCell\n",
    "    rnn_fw = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    rnn_bw = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "    rnn_outs, rnn_states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                cell_fw=rnn_fw,\n",
    "                                cell_bw=rnn_bw,\n",
    "                                inputs=embedding_input,\n",
    "                                sequence_length=seqlens, dtype=tf.float32)\n",
    " \n",
    "    out_fw, out_bw = rnn_outs\n",
    "    state_fw, state_bw = rnn_states\n",
    "    \n",
    "    out_fw = tf.gather_nd(out_fw, tf.stack([tf.range(batch_size), seqlens-1], axis=1))\n",
    "    out_bw = tf.gather_nd(out_bw, tf.stack([tf.range(batch_size), seqlens-1], axis=1))\n",
    "    \n",
    "#     print(out_fw)\n",
    "    rnn_out = tf.concat([out_fw, out_bw], axis=1)\n",
    "#     print(rnn_out)\n",
    "    \n",
    "    out = tf.nn.dropout(rnn_out, dropout_keepprob)    \n",
    "\n",
    "    # Linear activation (FC layer on top of the LSTM net)\n",
    "    y = tf.layers.dense(out, 1, activation=None)\n",
    "    y = tf.reshape(y,(batch_size,))\n",
    "    \n",
    "    return x, seqlens, y, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Taylor/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /Users/Taylor/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    }
   ],
   "source": [
    "x, seqlens, y, y_true = build_graph_bidir(rnn_size=8, embedding_size=10, dropout_keepprob=0.7) #batch_size=\n",
    "\n",
    "preds = tf.nn.softmax(y)\n",
    "label_predictions = preds > 0.5\n",
    "correct = tf.equal(tf.cast(label_predictions, tf.int32), tf.cast(y_true, tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "ETA = 0.01\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_true))\n",
    "optimizer = tf.train.AdamOptimizer(ETA).minimize(loss) \n",
    "# optimizer = tf.train.RMSPropOptimizer(ETA).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global data! - gensim indexes\n",
      "(epoch 0)\t Train: 0.63287, 0.49325 \tTest: 0.56135, 0.53115\n",
      "(epoch 2)\t Train: 0.29969, 0.56034 \tTest: 0.47250, 0.59672\n",
      "(epoch 4)\t Train: 0.20060, 0.56920 \tTest: 0.47774, 0.60000\n",
      "(epoch 6)\t Train: 0.14366, 0.57637 \tTest: 0.58261, 0.60000\n",
      "(epoch 8)\t Train: 0.10701, 0.57511 \tTest: 0.65026, 0.60656\n",
      "(epoch 10)\t Train: 0.08950, 0.58397 \tTest: 0.58054, 0.60492\n",
      "(epoch 12)\t Train: 0.08199, 0.58228 \tTest: 0.85413, 0.59836\n",
      "(epoch 14)\t Train: 0.07426, 0.58650 \tTest: 0.86435, 0.60656\n",
      "(epoch 16)\t Train: 0.05752, 0.58650 \tTest: 0.97854, 0.60164\n",
      "(epoch 18)\t Train: 0.06786, 0.58481 \tTest: 1.04995, 0.58197\n",
      "(epoch 19)\t Train: 0.08466, 0.58608 \tTest: 0.85015, 0.59836\n"
     ]
    }
   ],
   "source": [
    "run_metrics = run_graph(x, seqlens, y_true, optimizer, loss, accuracy, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A little improvment over a single LSTM here, but once I move on to longer text it may begin to make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
